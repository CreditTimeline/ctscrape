================================================================================
CreditTimeline Scraper (ctscrape) - Implementation Roadmap
================================================================================

Browser extension that scrapes credit report data from provider websites,
normalises it to the ctspec CreditTimeline format, and sends it to a ctview
instance for long-term storage and analysis.

This file tracks the full implementation timeline. Future LLM sessions should
read this file to understand what's been done and what comes next.
It should represent the current status of the project and you must keep this
document up to date at all times.

Whenever you complete a task in this list you must mark it as completed with
a x in the box. When an entire phase is completed mark the phase as completed
in the phase heading.
The user will move the completed tasks to todos-completed.txt (do not consume
this document unless you require historical context on why something was done)

Whenever designing or implementing features or functionality or bug fixes the
below ADRs must be respected and complied with.

================================================================================
ARCHITECTURE DECISIONS
================================================================================

AD-001: WXT Framework with Svelte
  - WXT (wxt.dev) as the extension build framework — Vite-based, auto-generates
    manifest from file conventions, handles cross-browser differences
  - Svelte 5 (runes syntax) for popup/sidepanel UI — consistent with ctview
  - TypeScript strict mode throughout
  - @wxt-dev/module-svelte for Svelte integration

AD-002: Manifest V3 (Chrome-first)
  - Target Chrome with Manifest V3 (MV2 is deprecated/removed)
  - Service worker for background processing (no persistent background page)
  - Content scripts for DOM extraction on target sites
  - Side panel as the primary UI (persistent alongside page)
  - Popup as quick-action entry point
  - Firefox support planned for later via WXT's cross-browser build

AD-003: Adapter Pattern for Multi-Site Support
  - Each credit report site gets its own "adapter" module
  - Adapters implement a common interface: detect page → extract raw data
  - Adapters are registered in a central registry
  - Content script loads the appropriate adapter based on URL match
  - New sites are added by creating a new adapter — no core changes needed

AD-004: ctspec as Canonical Output Format
  - All extracted data is normalised to the ctspec CreditTimeline JSON schema
  - Reference schema: ctspec/schemas/credittimeline-file.v1.schema.json
  - Reference enums: ctspec/schemas/credittimeline-v1-enums.json
  - Reference mappings: ctspec/mappings/ (crosswalks, normalisation rules)
  - Extension ships with a copy of enum values and mapping tables for
    offline normalisation

AD-005: @ctview/sdk for API Communication
  - Use the official @ctview/sdk TypeScript client for sending data
  - Handles auth headers, response envelope parsing, error types
  - Extension stores ctview connection settings (URL, API key) in
    chrome.storage.sync so they persist across devices

AD-006: Content Script Extraction, Service Worker Orchestration
  - Content scripts run in target site context to access the DOM
  - Content scripts extract raw data and send it to the service worker
    via @webext-core/messaging (type-safe protocol)
  - Service worker handles: normalisation, validation, storage, API calls
  - Separation keeps content scripts lightweight and focused on extraction

AD-007: Shadow DOM for Injected UI
  - Any UI injected into target sites (scrape buttons, status indicators)
    uses WXT's createShadowRootUi for CSS isolation from the host page
  - Prevents style conflicts with credit report site CSS

AD-008: Local Storage Strategy
  - chrome.storage.sync: User settings (ctview URL, API key, preferences)
  - chrome.storage.local: Scraped data queue, extraction cache, adapter state
  - chrome.storage.session: Temporary scrape-in-progress state
  - WXT's typed storage wrappers (wxt/storage) for all access

AD-009: Provenance & Artifact Tracking
  - Every scrape produces a RawArtifact entry with:
    - SHA-256 hash of the original HTML
    - Timestamp of extraction
    - URL of the source page
    - Adapter version used
  - acquisition_method is always "html_scrape" for browser extension data
  - source_wrapper records the site name (e.g., "CheckMyFile")

AD-010: Validation Before Send
  - Extracted data is validated against ctspec JSON schema before sending
  - Validation errors are displayed to the user in the UI with clear
    descriptions of what's wrong
  - User can review and optionally edit/correct data before sending
  - Invalid data is never silently sent to ctview


================================================================================
FUTURE: ADDITIONAL ADAPTERS
================================================================================

Each adapter follows the same pattern established by the CheckMyFile adapter.
These are listed in priority order based on UK market coverage.

ClearScore (TransUnion data):
  - [ ] Research ClearScore page structure
  - [ ] Implement ClearScore adapter (src/adapters/clearscore/)
  - [ ] Section extractors for ClearScore-specific layout
  - [ ] Test fixtures and unit tests
  - [ ] Integration test: ClearScore HTML → valid CreditFile

Credit Karma (TransUnion data):
  - [ ] Research Credit Karma UK page structure
  - [ ] Implement Credit Karma adapter (src/adapters/creditkarma/)
  - [ ] Section extractors for Credit Karma-specific layout
  - [ ] Test fixtures and unit tests
  - [ ] Integration test: Credit Karma HTML → valid CreditFile

Experian Credit Expert (Experian data):
  - [ ] Research Experian page structure
  - [ ] Implement Experian adapter (src/adapters/experian/)
  - [ ] Section extractors for Experian-specific layout
  - [ ] Test fixtures and unit tests
  - [ ] Integration test: Experian HTML → valid CreditFile

MSE Credit Club (Experian data via MoneySupermarket):
  - [ ] Research MSE Credit Club page structure
  - [ ] Implement MSE adapter (src/adapters/mse/)
  - [ ] Test fixtures and unit tests

Generic Page Scraper:
  - [ ] Implement user-configurable scraper for unsupported sites
  - [ ] Allow users to define CSS selectors for each data field
  - [ ] Template system for saving and sharing scraper configs
  - [ ] Import/export scraper configs

================================================================================
FUTURE: ADVANCED FEATURES
================================================================================

Auto-Scrape & Scheduling:
  - [ ] Auto-extract when a supported report page is loaded
  - [ ] Configurable: prompt before extracting vs. silent extract
  - [ ] Optional auto-send (with confirmation) vs. manual send
  - [ ] Chrome alarms API for periodic check reminders

Diff Detection:
  - [ ] Compare new extraction with previous extraction for same subject
  - [ ] Highlight changes: new accounts, closed accounts, balance movements,
        score changes, new searches
  - [ ] Show diff summary before sending to ctview

Multi-Subject Support:
  - [ ] Support extracting data for multiple subjects (household members)
  - [ ] Subject selection/creation in the UI
  - [ ] Associate extractions with correct subject

Offline Queue:
  - [ ] Queue extractions when ctview is unreachable
  - [ ] Background sync when connection is restored
  - [ ] Queue management UI (view, delete, retry pending sends)

Data Enrichment:
  - [ ] OCR support for image-based data (screenshot regions)
  - [ ] Manual data entry for fields the scraper couldn't extract
  - [ ] Correction/annotation before sending

Firefox Support:
  - [ ] Build and test with WXT's Firefox target
  - [ ] Handle Firefox-specific API differences (sidebar vs sidepanel)
  - [ ] Submit to Firefox Add-ons (AMO) with source ZIP
  - [ ] CI/CD for Firefox builds

CLAUDE SHOULD IGNORE BELOW THIS LINE!

  ---
  What you still need to do

  1. Faro collector URL — Replace the empty string in src/lib/telemetry/config.ts:8 with your Grafana Cloud Faro collector URL
  2. GA4 setup — Follow docs/ga4-setup.md to create the GA4 property and get credentials
  3. GitHub Secrets — Add these to your repository:
    - GA4_MEASUREMENT_ID, GA4_API_SECRET
    - FARO_API_KEY, FARO_APP_ID, FARO_STACK_ID, FARO_SOURCE_MAP_ENDPOINT
  4. Local .env.local — For dev testing: GA4_MEASUREMENT_ID=G-xxx and GA4_API_SECRET=xxx

prompt text (already at start of instruction)
Consume todos.txt and follow all instructions within it. We are now ready to plan out and implement Phase 5. Please make a comprehensive implementation plan for all tasks listed within the phase, ensuring we are comforming with all defined ADRs in todos.txt. Make use of subagents and agent teams where appropriate during the planning and implementation phases once the plan is approved. Ensure the todos.txt file is updated to mark each phase as complete once it is completed and ensure this initial prompt is included on any plan produced so it is not lost during context compaction.